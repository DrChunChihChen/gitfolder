import pandas as pd
import re, time, requests
from selenium import webdriver
from bs4 import BeautifulSoup
import re
import csv

lst = ['讚', '·', '回覆']

url = 'https://www.facebook.com/groups/599492557498340'

script1 = """
var elements = document.getElementsByTagName("span");
for (var i = 0; i < elements.length; i++) { 
    if (elements[i].innerText.includes('檢視另') || elements[i].innerText.includes('查看另')) { 
        elements[i].click(); 
    }
}
"""

click_login = """
var elements = document.getElementsByClassName('a8c37x1j ni8dbmo4 stjgntxs l9j0dhe7 ltmttdrg g0qnabr5');
for (var i = 0; i < elements.length; i++) {
    if (elements[i].innerText.includes('登入')) {
        elements[i].click();
    } 
}
"""

# 查看更多留言
script2 = """
function check() {
    var elements = document.getElementsByTagName('span');
    var count = 0;
    for (var i = 0; i < elements.length; i++) {
        if(elements[i].innerText.includes('查看更多')) {
            count++;
            elements[i].click();
        }
    }
    return count;
}

var x = setInterval(
    function() {
        var count = check();
        if (count == 0) {
            clearInterval(x);
        }
    }, 5000
)
"""


def login():
    global driver
    driver = webdriver.Chrome(r'D:\pythonProject10_GWO\chromedriver.exe')
    driver.get(url)
    driver.execute_script(click_login)
    time.sleep(2)
    email_element = driver.find_element_by_id('email').send_keys("你帳號\t")
    pwd_element = driver.find_element_by_id('pass').send_keys('密碼自己\n')
    driver.get(url)


def save(s):
    with open('data.txt', 'w', encoding='utf8') as writer:
        writer.write(s)

def get_page(src='facebook'):
    if src == 'facebook':
        login()
        driver.get(url)
        n = 3
        for i in range(n):
            time.sleep(3)
            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')

        driver.execute_script(script1)
        time.sleep(3)
        driver.execute_script(script2)
        save(driver.page_source)
        return driver.page_source
    else:
        with open('data.txt', 'r', encoding='utf8') as reader:
            return reader.read()

    # with open('data.txt', 'w', encoding='utf8') as writer:
    #    writer.write(driver.page_source)
    # return


regex = re.compile('分享對象：花花當沖分享社的成員(.*?)[0-9 ]+則留(.*?)(?:分享對象：)', flags=re.DOTALL)
import csv


def main():    
    i=0
    with open('output1104.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['排序', '時間','內容','留言'])
        page = get_page()          # 'facebook'
        #page = get_page('data.txt') # 'from data.txt'
        soup = BeautifulSoup(page,'html.parser')
        articles = soup.find_all('div', {'role':'article','class':'lzcic4wl'})
        print(len(articles))
        text = ''
        for article in articles:
            main_article = ''
            sub_article = ''
            posttimes = article.find_all('b',class_='b6zbclly myohyog2 l9j0dhe7 aenfhxwr l94mrbxd ihxqhq3m nc684nl6 t5a262vz sdhka5h4')
            for posttime in posttimes:
                if (posttime.text != "="):
                    posttime_out= posttime.text     
            if article.has_attr('aria-posinset'):
                main_article = article.text
            else:
                sub_article = article.text

            writer.writerow([i, posttime_out,main_article,sub_article])
            i+=1
        # 開啟輸出的 CSV 檔案
    
        for each in regex.findall(text):
            print(each[0])
            print(*[re.sub('· ([\d ]+小時|[\d ]+分鐘|[\d ]+天)', '', x) for x in each[1].split(' · 讚 · 回覆')], sep='\n')
            print()
        
    input()
    
main()
#-----------------------------------雄哥--------------------
#把檔案弄出來 切
data = pd.read_csv("output1104.csv")
df = data #看你要不要df 我是怕動到原始檔案
df[['內容','切']] = df['內容'].str.split('讚留言',expand=True)
